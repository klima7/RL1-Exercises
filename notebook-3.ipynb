{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Laboratorium 3 (4 pkt.)\n",
    "\n",
    "Celem trzeciego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmów uczenia aktywnego. Zaimplementowane algorytmy będą testowane z wykorzystaniem wcześniej przygotowanych środowisk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dołączenie standardowych bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dołączenie bibliotek ze środowiskami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from env.dqnSimpleMDP import dqnSimpleMDP\n",
    "from env.CliffWorldMDP import CliffWorld\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Zadanie 1 - SARSA($\\lambda$) (1 pkt.)\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenie jest zaimplementowanie algorytmu SARSA($\\lambda$). Algorytm aktualizuje funkcję wartości stanu-akcji dla każdej odwiedzonej pary stan-akcja zgodnie ze wzorem:\n",
    "\\begin{equation}\n",
    "    Q_{t + 1}(s, a) = Q_t(s, a) + \\alpha \\delta_t E_t(s, a)\n",
    "\\end{equation}\n",
    "gdzie:\n",
    "    \n",
    "- $\\delta_t = r_{t + 1} \\gamma Q_t(s_{t + 1}, a_{t + 1}) - Q_t(s_{t}, a_{t})$,\n",
    "    \n",
    "- $E_t(s, a)$ - ślad dla pary stan - akcja w chwili czasowej $t$, zwiększany o $1$ w chwili odwiedzenia danego stanu.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class SARSALambdaAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions, lambda_value):\n",
    "        \"\"\"\n",
    "        SARSA Lambda Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "\n",
    "        Functions you should use\n",
    "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
    "            which returns legal actions for a state\n",
    "          - self.get_qvalue(state,action)\n",
    "            which returns Q(state,action)\n",
    "          - self.set_qvalue(state,action,value)\n",
    "            which sets Q(state,action) := value\n",
    "        !!!Important!!!\n",
    "        Note: please avoid using self._qValues directly.\n",
    "            There's a special self.get_qvalue/set_qvalue for that.\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self._evalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self._visited_state_actions = []\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.lambda_value = lambda_value\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
    "        self._qvalues[state][action] = value\n",
    "\n",
    "    def reset(self):\n",
    "        self._evalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self._visited_state_actions = []\n",
    "\n",
    "    # ---------------------START OF YOUR CODE---------------------#\n",
    "    def get_evalue(self, state, action):\n",
    "        \"\"\" Returns E(state,action) \"\"\"\n",
    "        return self._evalues[state][action]\n",
    "\n",
    "    def set_evalue(self, state, action, value):\n",
    "        \"\"\" Sets the Evalue for [state,action] to the given value \"\"\"\n",
    "        self._evalues[state][action] = value\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        You should do your SARSA-Lambda update here:\n",
    "        \"\"\"\n",
    "\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to update value in the state for the action \n",
    "        #\n",
    "        next_action = self.get_action(next_state)\n",
    "\n",
    "        delta = reward + gamma * self.get_qvalue(next_state, next_action) - self.get_qvalue(state, action)\n",
    "        self._update_current_state_action_trace(state, action)\n",
    "        self._update_past_state_actions(delta)\n",
    "\n",
    "        return next_action\n",
    "\n",
    "    def _update_current_state_action_trace(self, state, action):\n",
    "        current_value = self.get_evalue(state, action)\n",
    "        new_value = current_value + 1\n",
    "        self.set_evalue(state, action, new_value)\n",
    "        self._visited_state_actions.append((state, action))\n",
    "\n",
    "    def _update_past_state_actions(self, delta):\n",
    "        for state, action in self._visited_state_actions:\n",
    "            e_value = self.get_evalue(state, action)\n",
    "            q_value = self.get_qvalue(state, action)\n",
    "            new_qvalue = q_value + self.alpha * delta * e_value\n",
    "            new_evalue = self.discount * self.alpha * e_value\n",
    "\n",
    "            self.set_qvalue(state, action, new_qvalue)\n",
    "            self.set_evalue(state, action, new_evalue)\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the best action to take in a state (using current q-values).\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to get best action for a given state\n",
    "        #\n",
    "        #\n",
    "        qvalues = [self.get_qvalue(state, action) for action in possible_actions]\n",
    "        best_qvalue = max(qvalues)\n",
    "        best_actions = [action for action, qvalue in zip(possible_actions, qvalues) if qvalue == best_qvalue]\n",
    "        best_action = random.choice(best_actions)\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.\n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "\n",
    "        Note: To pick randomly from a list, use random.choice(list).\n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "\n",
    "        # Pick Action\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters:\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to get action in a given state (according to epsilon greedy algorithm)\n",
    "        #\n",
    "        should_random = random.random() < epsilon\n",
    "\n",
    "        if should_random:\n",
    "            chosen_action = random.choice(possible_actions)\n",
    "        else:\n",
    "            return self.get_best_action(state)\n",
    "\n",
    "        return chosen_action\n",
    "\n",
    "    def turn_off_learning(self):\n",
    "        self.epsilon = 0\n",
    "        self.alpha = 0\n",
    "\n",
    "    def display_qvalues(self):\n",
    "        for s in self._qvalues:\n",
    "            print(\"State: \" + str(s) + \" \" + str(self._qvalues[s]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " Czas nauczyć agenta poruszania się po dowolnym środowisku:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def play_and_train(env, agent):\n",
    "    \"\"\"\n",
    "    This function should\n",
    "    - run a full game, actions given by agent's e-greedy policy\n",
    "    - train agent using agent.update(...) whenever it is possible\n",
    "    - return total reward\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    state = env.reset()\n",
    "\n",
    "    agent.reset()\n",
    "\n",
    "    done = False\n",
    "    action = agent.get_action(state)\n",
    "\n",
    "    while not done:\n",
    "        # get agent to pick action given state state.\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # train (update) agent for state\n",
    "        action = agent.update(state, action, reward, next_state)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD6CAYAAABd9xscAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAwuUlEQVR4nO3dd5xU5bnA8d8z2xe2AQtsYWHpva4IiKKCSgRFTDRqrDHBGpPcxIIlJrkx12tMcmMSC6lqEk1urLHEEqPRK4SioqAiLFKW3lkXdpfdfe8fc2Z2+p7pZZ/v57Mwc+p7Zs6c57z1iDEGpZRSyg5HshOglFIqfWjQUEopZZsGDaWUUrZp0FBKKWWbBg2llFK2adBQSillW9oFDRGZKyLrRGSDiNyS7PQopVR3IunUT0NEsoBPgNOABmAFcKEx5sNg6/Tp08cMGjQoMQlUSqkMsWrVqr3GmHLf6dnJSEwUpgIbjDEbAUTkcWABEDRoDBo0iJUrVyYoeUoplRlEZHOg6elWPFUFbPV432BNU0oplQDpFjQkwDS/8jURWSQiK0Vk5Z49exKQLKWU6h7SLWg0AAM83lcD230XMsYsMcbUGWPqysv9iuSUUkpFKN2CxgpgmIjUikgucAHwbJLTpJRS3UZaVYQbY9pE5HrgJSAL+K0xZm2Sk6WUUt1GWgUNAGPMC8ALyU6HUkp1R+lWPKWUUiqJ0i6noZw27vmMnYeamTG0j3va/qZWltbvY87ovjzz3nbOm1KNSGeDsxWb9rNi034+a27jP04bzsNLNzNjSG9GVRT7bf+t9Xt5fd1ubvncSLKznPcWH+04zKa9TTQcOMrm/U0sOnEIb6zfw/iqEvJyHHy6p4mWtg52HW4myyGMry7l+fe3c+7kat6u30d1WQE5WcLcsRX834a9PLp0M/1L8rn1zFE8tnwLI/sX8e7Wg7S2dXDxtIHkZjv44QsfsXBSFf/4aDd1A8sozMtiWf0+th9qprK0gPW7GinKz+bw0TZysh3kZAkDygpZMLGSfU2tvLP5AHNG9+ODhkN8tOMw2w4eZUS/ImYO68OfV2ylpCAHESEnS9hxqJmaXoW0tXfwya7POHtiJcfaO5ha24sn39nG0vp9VJbmU16Ux8Y9TRTkZvHt00fwt9XbOdLazqZ9TQwoK2T7oaNgoLK0AIdAW4ehX3E+l04fyAsf7OTlD3cya3g59Xs+Y29jKx3G0Lc4j5wsB2dPqOTFNTtpbG7jtNF9eeKdbVSXFVDeM48t+4+wcW8Tl04byPsNh9h28CgiMHFAKbV9evDnFVspL8rj0NFjzB9fySNLNzGifxFlhbm8tHYnp43uR1F+Dkvr93LmuArebzjE6+t2c3xtb9qNYUBZIfubWjh6rJ1sh4OtB45QUZLPxj1NnDKyL/s+a6W1rYPyojzWbD9EaUEO+5tauWP+aFraOvj5a+vJzXKwu7GFk4b3wSFCbpaD97cdIschiAiNzW3Ulvdg56GjjKooprWtg5fX7qJnfjZTB/Vi+6GjnF83gMrSAl74YAetbR2s2LSfSTVlbNnXRIeB7YeOMqCskEF9Cln+6QEmVJewdvthjrV3sPXAERZMqGJfUyt9i/JYv/sz3tlygHnjKthxqJnN+5ooyMmiojSfnnk5lBTksPPQUQBGVhTTrzgfMIyuKOHp97aRn+NgSHlPfvXmp5w7uQoB1m4/zOcnV3PPSx+T7RDGVZfyQcNBygpzGdq3J29t2EvLsQ5OH9OPTfuOUL/nMwb2KkQEjrS2U11WyNL6fRQXZHO0tZ2R/Yv50rQadh5q5uUPd5HjEC6dPogX1uzgxTU7WTipkt2HW+jTM49N+5oQEc6eUElxfjYrNx+gplchL6/dyfZDzeRkCdVlhUwb3Jul9Xu5/IRaeubF9jKfVj3CI1FXV2cysXPfoFueB+DD75/B4ic/4NYzR/Gtv6zmrQ17+fzkap54p4FfXjSZeeMrAHhk6Sa+80xn9c/lMwbx+7c3MXFAKRcdX0PzsXYunT4IgLb2Dobe9iIAt88bxZUzaznWbhh++4sxSfvoimI+3HE45DKDy3vQsP8ore0dEe1jysAyVm0+ENG68fL8DTOZd99bIZcZ0a+IdbsaE5Si1NSnZx7Th/Tmb6v9GkYmzMJJVTz17raE7W/++Aqee3+H7eUvOG4An+xq5J0tB+nTM4+9n7V4zReBnrnZLLt1Nj0iDBoissoYU+c3XYNGcr2+bjeX/24Fb950CgN6Fdpa556/f8z9r9cD8F/njmPxkx9wwXEDWLX5AOt3f8bkmlLe2XLQvfxvLqvjyoe9P4PSwhwOHjnmNW3T3fN4+O1N3Plsarct+NqpQ/n5axuSnQxbRlUU81EXAVJ1qiotYNvBowHnfWf+aL7/XNDBH6IS6MIbC+EGAzuyHEJ7R9fX7bFVxTz3tRMj3k+woKF1Gkn25xXODu4n3vNP3tlygFWb9/Pzf6ynI8RJ4QoYAA9Yr595bzvrd38G4BUwAL+AAfgFDIDLf7c8YQHjB+eM5b4LJ0W0rmdxWqCitVAevHhywOm/vbyOS6cPBJw/9D8vmhZ0G3eeNZp7Pj+er55Yy01zR/DUtTO4ee5IvjN/tNdy88dXcN0pQ/zW/+/Pj+OGU4fy4MVTwkp7KJNrSgEoL8ojy+Eskhxc3sPWuk9cM4N7Pj8egMqSfL/5N8we5jdtzqh+trZ989yRPHjxZC6cWuM374ShvamxbpTu+cJ4vnf2GJ66dgZD+/YMuK2e+f53zNMH9+Z3VxzH37/ReXE8cVgfbp83ivOmVAPOXHVXLpo6IOi86rIChvfzT1OxR3qWLZ7NjWeM8Fsm0DQXh8AtnxvJrOHefcmm1vZi7pj+XHR8DUsumcJtZ45yz1s4qSpgD2fX9+9p874jQfcdDa3TSLJj7Z3B4dz736ayJJ/th5qZP6GS2j5d/+i37HeeGEePtUedltfXJab3/KNXTuXEYc4fSsOBI9zz93VexQEDehXw+yumUlaYy3tbD/Dl33sHPWe5s9Oik2r55p9X2973pJoyr/fjqkq4ZNpATh3Zj1NH9uOG2cMoK8wlyyEsv3U2vXvmMeRWZ2O9/sX5XDmzlitOqA263S/UVXPDY+/y+ro9fG5sBfPGVzCppowT7n4NgNkj+/LF45wX0MPN/oEbINshnDyiL5v2NXHXOWOpKCnggiVLue7UofzitQ1cPG0gz7y3jf+9egbZDqGppY2+xfns+6yFssJcrvrDKl75cBfnTRmAQ+DRZZsZUt6TNz7x/36rSguYMrCMKQPLOG10Pz7Z1cgXlywDnBfk+y6cRHlRHjkO4cevfMJFx9fwjdnD6Fucz+wfv079niYAzq+r5ooTarn41//mm6cN55f/3MB/LhjLnNHO4DJnVD/6FuXxyoe7eGzRNFrbOujTMxeAfU2t9OmZ507TVScN5sa/vs+YymL2N7Xy/QVjufvFj5gxpLdX2gf1LuTBi6dQUpgDwE+/OIFf/rOeR7481V2Xd9PckZQX5TGpppSvP/4eAJdMG0hbh+Gx5Vvc26ou887lDy7vwUbr2K6eNYTx1SVc+fBK/viV41lav487n13LXQvHcbS1nWdWb6N/ST7nTq7iseVb+OkXJzKuqoSmljZ698zjB+eM5fan1wDwzTnD+emrnwAwa3g5V88awp/+3ZmOp687gXFVJe7A7zJ5YBnX/fEd7jxrtPt3kpvtoLXNWXx718Jx9C3KIzvLwYTvvRzwvIoVDRpJdsynzH77oWYAjrZ6B4H9Ta3saWxh2cZ9CUtbOI6v7cW88RXuepM3bjyZWT963T3fVc/ylZm17oABcO3JQ7n25KEAPPXuNorysnnzplPd82cN78vVs4Zw2YyB/OzV9TxuVV6vun0OS/61kXnjKjnS2k5jcxt3v/hxyDROG9yLvkV5XtP+9rWZXu89L159reA0sn8RH+9s5O1bTsXhCHSf16k4P4dxVSW8vm6P+6JYVVrAE9fM4PMPvM3w/kXuZYuClDX/zwUTmT++0mva24tnA/Cl4525oetOGeqe5yqz7m2l3VXinOWARScN4apZztzOoSPHmPB97wuK5/GX9cilblAvKkry2XGomX7FeZRbn9fXZg/jaz45jl9fdhyn3Ps6AIW52YyqKGbVHacBcPG0gV7LZmc5+OZpw/nmacP9jtfzMwc4a0Iln+xq5PpThrkDwmmjvXM2a753hl8F78JJ1SycVO01zZX+BROrONraTm2fHhw/2Bl8RlcU8eS723h3y0FGV3bmWF//9skM6tODU+99nY17mxCB8dWlrLhtDgDD+xVxmUfu5fzjnLmUipIC3rq589zNz8kCOm9ysh3C1+cM46ThfVh4/9sM7+c8Fw4eaXWvU1ma7xcwwFlHt+zW2V7TVtw2xx0gevXIdX//LqHP1Mhp0LChvcPwi9c2cMXMQRTn5/jNbz7WzgOv13PtKUPIy3aeKHs/a+HPK7Zy7clDvFowBdq2p2yH0NZh3DmHnYeaeeKdBv62ejsf72xk6qBeMTuu1741i9ufXsPb9Z2B6M2bTuEHz3/IS2t3uac9fd0J/OC5D1lpVSzPGNLbax2ARScNpqqswP1+YO8e5GSJOyc1tbaMhZOqOK7W+07f09+un0m/Yu8TP8sh3PK5kQB8b8EY5o+vdBdfLLay7V86fiBrth0CoEduFo9cOZU9jS08/8FOr8rUe8+bEPK7COaxr05j076mLgOGyw2zh3HcoF7uixM4f/SPfHkq0z3ulkWEp66dwaDePViz/RD7m1r5+uPvMaJfUaDNhk18LhslhTksXXwqB48c43M/exOAHnlZXstkOYRvzhnOTU+83+Xx1vbpwXWnDOGX/6wngo81qPycLG6bNzrgvMumD+ThpZvpkZsVcH4oF/gUkV0yfRBfmDKAd7ccYGxViXu661hcxWG+n2O4crMdXv9Pqinj0Suncnyt81y48PgaHnpjIwAFOfaPy7N4rKTA/7oUybluhwYNG15au5OfvvoJOw4d5W6r7NfTb976lJ/9Yz2FuVlcNWsIbe0d3PDYu7xdv49pg3szZaD3hdIYQ1NrOz3zsv1yGu3WbeJnLW00tbRx419X8+b6vTE9nsF9enDT3BEMLu/J/PGV7gDw7PUnMKCXM8vf0tbByDv+DkBRfjY/OX8iix5dya8urSM328HxP/yH1zZFcAdMlwe+NIWvPOIsWuowMHNYH0IZV10Scn5edlbQbeTnOH+QvXrmMmWgM7BOre3N+l2N3P+lyVSWFrjv/Fy+MtO/mCmQsh65lPXItbUsQE6Wg5OG+495Fmiaq1jLlfs6fXR/CiK4IAYS6JpRUVJARUkBD148mceWb/X7zgDyrM8y0IXIbx/WBTXaC6tdd541hlvnjYrZBbEgN8ur2Tr4H0u0u8rN8g4agFdu++YzRrqDhu85Gsjd545j6cZ9Xp9BoPXiFDM0aNjhKjc80hq43qDFyhU0H3Mud8lvlrPUKkYK1Mrhr6sauPGv7/Pat2b5BQ1X0cJlv10OwASfC2l7lK3dzhjTj4cu6WwQ4brTLC/KY3x1KeC8Q/E8CQWo6V3I379xknvaB989nUeWbubel9dhjPOH5ntjOmd0Py6eVsMflm1xf0bxku1wWGntTESvHrleafblWcSTKmIVMLoyd2wFc8dWBJw3b1wFW/cfCVh348t1YYrXBcqXwyHkOeL7GfkeS7SHlpvt3IIrePjyzNHlBFnG0wVTa9y5pr9/40TWbEts6zxtPWVDuD+IpV3UO6zc5Czmuf5P7/q1dPLlCkQuwU68YH55kXdrobZ276DjKj8NFYscAT6AovwcrjtlKFnuq0bgu83cLOcP/Fh7fJt2R3LRStSFLtE6L+SRHWB2loPrTx1mq32/+PyfiaLPaTh/A3YCQrhG9i/mC1OqA86L13eiQSMMrsve7sZmAvVv+emrn/AVn+atjc3H3JXah5uP0dTS5q6c66qDG0Bzm/cdeltH153dFp002P3a1bnP5ZhPzsdOsYKdH40EWe7qkwdz8ohyzqsLfGLHSnVZIWeO68/Pw2jGm6gilWRJyNFZX3omBWD/nEZ0B5fjymlkB7/cPnTJFFtNg8OhdRopwBjD5n1NzPrR69w8dyTXnGy1wff4cl79aJfXOlc+vJIBvQp486ZTGf/dlynKz+aLdcHbhPvybWtt54791jNHMW9cBU0tbX7z2nyKw+zU7doLLIGX6VuUz++vmNr1TqKU5RDu/1KY/R4y6EIXSCIu5DbbBqQVv3M5ymN03V+GKiU4Y0x/zhjTP7od+dCcRhJ5nkQNB5y9Vd9cb79Pw9b9nT1cG5vbaGmLbGgMgPe2HrS13IQBpX4VfIB7qBAXW7kIG8s4JP3uNtMtveFKxOFlYm7NdUSui320R+iqt3Q11kh3mtMIg+c9fjT10c02K4Vr+/Tg071NYW071J3fprvnBZgamx+9IHHLDsdLeqXWPte5abeJcDSirT9JRb6HEqhOLxxjKku46PgarvIoNk5nmRH64kyCvI5Us82cxqQBpWFv29WKyNOg3sHHtOr8PQSPgnZzI+l22Yj2YpDqEpPTyDyxbnKb5RB+uHAcA3vbG9YlVuIVyDVoJMijSze5X9vNadjpG/DktTO83gfqTfri109i9XdOD7i+nQunnZNPbG4rlaRZcsOXgAPMxM/Qtxlxuh7jTSHGvYqGFk/FgJ1z6g6PYcl9hwgJxk7nKt9hMQIFjYLcrKDt/+2k3dZvJh3rNDLyPrlTQnIa6fal2+B7ROl6nswdF9uKdRfNaYTDeL6MvFLDc6C6U0aU+42941Ja2HXQiPbuPnYV4en308rA6x2Q/nfISefbeCpNP0dtPZVErpPGYLy+icbmY1y4ZBmrGw6GtT3PYclFhP+75ZSAy9kaxiHKM8NuQOhyO+5/VKpIRBhP1wtqKOl3+xOY9tNIomAn0dPvbuuy93cgBzxGtQT/MZt8p/fMy2bywDL+FWBo62hPcLv1FXa2k24/tky84HlKxPGl23duh183jTQ9UTSnkQI8m9ka4z/Eh12NzZ2d7lxf7CKf5nhv3XyK++TNcgjfmOP/IByIQU4jRguJ1mmknMTUaSRgJwnmX6eRnuL13WjQsMHzw/e80MTiS3Ft41aPp3OBc1gMz2KhYLuKNgn2chr2ciPp9uPKxAseePTTSETrqbjvIfF8fxPpep7E66ZIg0YYfDv0BWqpFL7g2/DqHxLszI1RTiNUZ8VQPxrjsUzaNblNdgLiLeMPMD4ypfWU5jSSyH1h9WkxFYuLZKBNXG8N2e3qp2eMCZHTiC4Ntiu5u1pGJO3uyNK1rNquRB5doAE805XvbyLDT5OwadCwIdBJY4Ad1qNZ7Qj2wJ9A5+O3rU45dorCoh1bzV6TW7vFU+n160qv1IYv04Ni3ET5m0oVmtNIAS+t3dV5R2XgwTfqba+bm+3g9nmj/Kb7frGuR5k6Z3q+7HwzuabU/TrqfhpRLuOaJ2k4jkimX1MTeXiZFKD8W08lJx3RitdNnDa5DdOflm8Bwu/cF7xKonPGqtvnePXc9qoI91h/4oAy98Oboj4tYtS5T2wul0oy6ULnSTv3Rcf/Y0vPD7Lb5DRE5Lsisk1E3rP+zvSYt1hENojIOhE5IxnpO3TU2TFvhfX0Pbucj0MN/S327plHYW5nHPeuCO987Tksf7Qnhr06DRvLpF9GI+Np0IiM62aiMxedvLREI17JTtWcxk+NMfd6ThCR0cAFwBigEnhVRIYbY+L78Gnn3qPeQiTPm+jsie594fZuihtdAayt4qkQtxau/JZD0m9o9EzV+RwI/T4ikTn9NLTJ7QLgcWNMizHmU2ADEP9HwhGjOw3pOqfhK1jxlNdzEqJtchuj1lOQmU9xS2cawyOjPcJDS9Wgcb2IvC8ivxWRMmtaFbDVY5kGa1pCRdqyMJIy/2DFU14xI+qg4fw/1GHZCiyid7apJl0vdsnm9zyNJKUjWhlVpyEir4rImgB/C4AHgCHARGAH8GPXagE2FfBaJyKLRGSliKzcs8f+Y1mDptdrh5FFjUjK/CVIMZSdnuJ22XtGuJ1l0q+fRqbTryMyGdN6KpMGLDTGzLGznIj8CnjOetsADPCYXQ1sD7L9JcASgLq6upTodRTJ41AlSI7CM2j4dUSKIGXhpCPY2un6w8pk+p3Ehn6O3lKueEpEKjzeLgTWWK+fBS4QkTwRqQWGAcsTlKYYbKPz5Dt3sr1SNfdevUdk9xq+JFbFU6H7YgSf6zmMiP64UosWF0bGv8Osfo6eUrH11D0iMhHn9WgTcBWAMWatiPwF+BBoA65LTMspn+KpaOo0cDXls3cSOoIEB+968OhOaFtFTzYWcqTh0OiZSvtpRCfaFomZLuWChjHmkhDz7gLuSmBy/Lxdb//5GSP6FbFuVyOANaBfePsKVvDkVdcRo34a0ZbhRVLRr+IrkV9HJo09Fe3QPJku5YqnMol3nUT4FcXBchexGV3Xfx/RLpNuo9xmKnc/Df0+IuLXT0M/Ry8aNOLIN0cQbvGN9/qeld+ey0SePrCXJnvBQAunUk0ir3WZdGH1e55GktKRqjRo2BDp78FruA/Cb3PrWtzgvWrIHuFhsjuulJ3tZNB1IyPo1xEZ/5xGUpKRsjRo2BBx0LBR9xCq30fQHuEhthtps95QZdJ2tqnDiKhM4T6N3WNQ6XntSYNGHHl3zvOcbnd9j9ceW/Cs04h+aHQb/TRsbUepzJApj3uNFw0aNkR6pxFt3UOwhzDFcOgpj+aZwbcUq8pylViZ054pufTU9qZBI468OuEhYf+Kg12Ig/XfiIS9gGAnN6I/LZWh9NT2okHDjghPmrljOzu3R5TT8KhvCF6nEd0Z7e6nEWU7e81ppA79LmJLb4i8adCwIZJT5obZwygvyuvcRgS/ZO/iqcBNbqMV7aYyqVNXptCvJLY0CHvToBEngk/ld5DXoTg8vh1HkJxGtGK1KYc+TCPl6DcSG/o5etOgESe+/RYiufnzymkEaT0VvWj7ebiaJSqVmbQpuTcNGnEi+A/gF+6zOIK1vgqV0wj3/HbtI+qxp/R31a1lcjGlntveNGjYEFF9hM8qnj+qcPtp+PUIj+nYU7HZllYWqkylZ7Y3DRpxEqtLcaCX8agIj1V/D5U6Ennvn8lFOBl8aBHRoGFDJOdMLE60YM/NiOVdfbTpdOWg9IelMpee3J40aNgQWR8LeyuFKgoO9tyMWOY0Yvc8Df1hpQoN4LHhMwSVsmjQSGHBhgtJxaKAFExSt5XBddIqBWjQsCGSu2j/ivDwf8yu/RoTPNcRreiHIdEmt6lKvxMVDxo04iQWxTUSJKcR2859Maqy16yGUt2CBg0bIh03yrtzX/hlBkHHmwq1Tpj7cPfTiLJIQ0OGUt2DBo04CXQRNe559i6xEiSrEdPiqSgv967WU/p8cKW6Bw0acRKoTiPsbQTZXqjrc6RP7ov6mq8xQ6luQYNGnPgPIRJkuRAX287msCZuraditSXNaKQebUSl4kGDRpyEzg2Ev41gj46Nlmu7WqehlLJDg0YKC9Z6KqY5jVg1udWsRsrRb0TFgwYNG4LdhedmB//4YnER9e6nEfXmguwjOp0V4dGnRSmV+jRoRGHl7XN4947TAs7zvYZGVBEeZOypkOuEvQ8d5VYpZV92shOQ6p5Y1cC3/nd1wHnF+TlB17N7LQ4VTLyfBW5ve+HSinClVDiSktMQkfNEZK2IdIhInc+8xSKyQUTWicgZHtOniMgH1rz7JEGF6L9+69OI1vPLaWAiGEbE43W8goZe7JVSYegyaFgX+CLr9e0i8qSITI5yv2uAc4F/+exrNHABMAaYC9wvIlnW7AeARcAw629ulGmwpa29I6L1fGOaZ8CIqPVUihf/aPBRqnuwk9O4wxjTKCIzgTOAh3FewCNmjPnIGLMuwKwFwOPGmBZjzKfABmCqiFQAxcaYpcZZ8/oIcE40abCrrSOytqixuIi6m8PiG0BST6oHte5I+2moeLATNNqt/+cBDxhjngFy45SeKmCrx/sGa1qV9dp3ekAiskhEVorIyj179kSVoNa2CHMaAaaFO/5UsEARj4tBtM941tZTSnUPdoLGNhF5CDgfeEFE8uysJyKvisiaAH8LQq0WYJrvI7I9pwdkjFlijKkzxtSVl5d3ldSQ2joiCxqxyGq4KsKF+PWDiFUOQftppB79RlQ82Gk9dT7O+oN7jTEHraKiG7tayRgzJ4L0NAADPN5XA9ut6dUBpsddW3uExVMx2LdrG8GiZiqYMaQPb23Ym+xkKKUSJGTQEBEHsNwYM9Y1zRizA9gRp/Q8C/xJRH4CVOKs8F5ujGkXkUYRmQb8G7gU+Hmc0uDlWMQV4SHnhr2NSCrPE+GhS6aw/eBRsrR8SqluIWQxkzGmA1gtIjWx3KmILBSRBmA68LyIvGTtby3wF+BD4O/AdcYYV53KNcCvcVaO1wMvxjJNwRyLOKfhXfBjjOGsCZWcOKwPN8weamsb7n4aJnTxz1UnDebWM0dGlM6i/GzOmlDJby4/LqL1e+RlM6xfUUTrKqXSj53iqQpgrYgsB5pcE40xZ0e6U2PMU8BTQebdBdwVYPpKYKz/GvGx81AzH+88HHmdRgDF+Tk8euXxMduey+IzR7H7cDM/fOHjsNd1OISfXzgp5mlSSmUmO0Hje3FPRQpa8Mu32HW4JeL1fVtKRdU4yU7JTxxKh86bUs2b67W+It1cPK2GVz/axYQBpclOSkaIdgToTNNl0DDGvCEiA4FhxphXRaQQyOpqvXQXTcAAiLB7R2A2thWPfhI/Om9CzLep4u/kEX3ZdPe8hOyrVw9n6/s+PePVCj95tEFgYF0GDRH5Ks6e2L2AITj7RzwIzI5v0tKb3X4PoZbKtiqX50+oCDj/uEFl4SYrISpK8pOdBJUgn59cTW62g/njK5OdFJUgdoqnrgOm4my1hDFmvYj0jWuqMkCHT1YjkoxHdpaDd+44jaJ8/69p9XdOJz839QYpfv+7p5PjSL10eTpvSjV7P4suJ6mcHA5hwcSg/WxVElw4tYaGA0fitn07QaPFGNPq8bCdbHSEgi7FqnjKlf33VVLoPcJuuL3N4yXUyL+pQovdlB1Dy3vy7paDFAe4aUtl/3XuuLhu386n8YaI3AoUiMhpwLXA3+KaqgzgewmPRWVaUV42jS1tXSylBbFKReKtm0+hsbnz9/Wf54xlwcQqbVLuw045wi3AHuAD4CrgBeD2eCYqE0Q7llMgE2tKY75NpZRTdVkhoyqK3e/zc7KYOaxPElOUmuy0nuoQkYdx1mkYYJ2JxxUxw3QY3zqN6D+y0ZXFvLl+b0a2VFFKpQc7rafm4WwtVY+z7KNWRK4yxiSkR3a68q3TiEWYvfH0EZwxpj9jKkui35hSSkXATp3Gj4FTjDEbAERkCPA8CRrGI1355jRiITvLweSa1Gxmq5TqHuzUaex2BQzLRmB3nNKTtvKyvT9KLcBTSmWioDkNETnXerlWRF7AOZCgAc4DViQgbWlj6eJTKczN5kBTK3/892Z+9eanfv004k6DlFIqAUIVT53l8XoXMMt6vQfQMhIPFSUFAJQU5JCf4xxhpcN4X8fjfU0vL8rjkmkDuWDqgK4XVkqpCAUNGsaYKxKZkEzR+Vxv7zAxviq+ldciwn+ek7BBgJVS3ZSd1lO1wNeAQZ7LRzM0eiZzPYuow3Q+D2NyTSlzRvcLuHyWjoqmlEojdlpPPQ38Bmcv8Ng9XCJDuQKFMYbZo/py+YxBXH9q8Icuae5AKZVO7ASNZmPMfXFPSYoZ0a+Idbsaw16vM6dhyMly8N2zx4RcvrwoL5LkKaVUUthpcvszEblTRKaLyGTXX9xTlmQlhTkRPcTGVaeR6MZTSimVCHZyGuOAS4BT6SyeMtb7jPWXq6YDsLR+Hxf+apnf/LuDjCTpcAcNjRpKqcxjJ2gsBAYbY1rjnZhUFKyeevqQ3iGX15ihlMpEdoqnVgOlcU5HygrWtinY41Ud7qChUUMplXns5DT6AR+LyArA/biz7tLk1uEIr0msQ+s0lFIZzE7QuDPuqUhhQXMaQWaI1mkopTKYnedpvJGIhKSqcPveOWzWaXxn/mhysrRjn1IqvdjpEd5I59BJuUAO0GSMKQ6+VuaQMKOG3dZTX55ZG3GalFIqWezkNLwekCsi5wBT45WgVBNu8dSs4eUALJxUFZ8EKaVUEtmp0/BijHlaRG6JR2JSUbCcRrDpg/r0YNPd8+KZJKWUSho7xVPnerx1AHVEOdK3iJwHfBcYBUw1xqy0pg8CPgLWWYsuM8Zcbc2bAvweKABeAL6eiGeVh9l4SimlMpqdnIbnczXagE3Agij3uwY4F3gowLx6Y8zEANMfABYBy3AGjbkk4JGzwfpjKKVUd2SnTiPmz9UwxnwE9iuZRaQCKDbGLLXePwKcQyKChsYMpZRys1M8VQ58Ff/naXw5TmmqFZF3gcPA7caYN4EqoMFjmQZrWtxp0FBKqU52iqeeAd4EXgXa7W5YRF4F+geYdZsx5pkgq+0Aaowx+6w6jKdFZAyBGzEFrc8QkUU4i7Koqamxm+TA2wpSPKXDhCiluiM7QaPQGHNzuBs2xsyJYJ0WrKFKjDGrRKQeGI4zZ1HtsWg1sD3EdpYASwDq6uqirLSPZm2llMosdgYsfE5Ezox7SnAWhYlIlvV6MDAM2GiM2QE0isg0cVaEXIozBxR3Do0aSinlZidofB1n4DgqIodFpFFEDkezUxFZKCINwHTgeRF5yZp1EvC+iKwG/gpcbYzZb827Bvg1sAGoJwGV4M60JmIvSimVHsLuER4LxpingKcCTH8CeCLIOiuBhD9QO1jM0CoNpVR3ZCen0a2FO/aUUkplMg0aXQg+BHpi06GUUqlAg0YXNDYopVQnW0FDRGaKyBXW63IR6TbjegdrPaV1Gkqp7qjLoCEidwI3A4utSTnAH+KZqFSixVBKKdXJTk5jIXA20ARgjNkOxLxFVarSAQuVUqqTnaDRag1BbgBEpEd8k5RaNKehlFKd7ASNv4jIQ0CpiHwV5xhUv4pvslKHBg2llOpkp3PfvSJyGs5RZ0cA3zHGvBL3lKWIYP00ynrkJjglSimVfLYe92qMeUVE/u1aXkR6eQzvkdECPblPH+eqlOqu7DxP4yrg+8BRoANn1wUDDI5v0lKDVoQrpVQnOzmNbwNjjDF7452YVKR1Gkop1clORXg9cCTeCUlVGjSUUqqTnZzGYuBtq06jxTXRGHND3FKVQrI0aiillJudoPEQ8BrwAc46jW4l26HDcymllIudoNFmjPmPuKckRWnMUEqpTnYuif8UkUUiUiEivVx/cU9ZitCchlJKdbKT07jI+n+xx7Ru0+Q2K1BHDaWU6qbs9AjvNsOgB6JBQymlOtnp3JcDXAOcZE16HXjIGHMsjulKGRozlFKqk53iqQdwPkPjfuv9Jda0r8QrUalEnxGulFKd7ASN44wxEzzevyYiq+OVoFT3q0vrkp0EpZRKGjtNg9pFZIjrjYgMBtrjl6TUdtrofslOglJKJY2dnMaNOJvdbsQ5WOFA4Iq4pkoppVRKstN66h8iMgznszQE+NgY09LFakoppTJQl8VTInIekGuMeR84C3hMRCbHPWVKKaVSjp06jTuMMY0iMhM4A3gYZ+sppZRS3YytinDr/3nAA8aYZwB91qlSSnVDdoLGNhF5CDgfeEFE8myuF5SI/EhEPhaR90XkKREp9Zi3WEQ2iMg6ETnDY/oUEfnAmnefaAcKpZRKODsX//OBl4C5xpiDQC+cLaqi8Qow1hgzHvgEa1wrERkNXACMAeYC94tIlrXOA8AiYJj1NzfKNCillApTl0HDGHPEGPOkMWa99X6HMeblaHZqjHnZGNNmvV0GVFuvFwCPG2NajDGfAhuAqSJSARQbY5YaYwzwCHBONGlQSikVvlQY9/vLwIvW6ypgq8e8BmtalfXad7pSSqkEstO5LyIi8irQP8Cs26zKdETkNqAN+KNrtQDLmxDTg+17Ec6iLGpqasJItVJKqVDiFjSMMXNCzReRy4D5wGyryAmcOYgBHotVA9ut6dUBpgfb9xJgCUBdXV3Q4KKUUio8SSmeEpG5wM3A2caYIx6zngUuEJE8EanFWeG93BizA2gUkWlWq6lLgWcSnnCllOrm4pbT6MIvgDzgFavl7DJjzNXGmLUi8hfgQ5zFVtcZY1z9RK4Bfg8U4KwDedFvq0oppeIqKUHDGDM0xLy7gLsCTF8JjI1nupRSSoWWCq2nlFJKpQkNGmH4yfkTul5IKaUymAaNMMwc2ifZSVBKqaTSoKGUUso2DRrh0CESlVLdnAYNpZRStmnQUEopZZsGjTCIlk8ppbo5DRpKKaVs06ARBn1WoFKqu9OgoZRSyjYNGkoppWzToBEGLZ1SSnV3GjSUUkrZpkEjDKI14Uqpbk6DhlJKKds0aCillLJNg0YYtHBKKdXdadBQSillmwaNMGg9uFKqu9OgoZRSyjYNGmHQUW6VUt2dBg2llFK2adBQSillmwaNcGjplFKqm9OgoZRSyjYNGmHQJrdKqe5Og4ZSSinbkhI0RORHIvKxiLwvIk+JSKk1fZCIHBWR96y/Bz3WmSIiH4jIBhG5T3TIWaWUSrhk5TReAcYaY8YDnwCLPebVG2MmWn9Xe0x/AFgEDLP+5iYstRaNUkqp7i4pQcMY87Ixps16uwyoDrW8iFQAxcaYpcYYAzwCnBPfVCqllPKVCnUaXwZe9HhfKyLvisgbInKiNa0KaPBYpsGallBaIqaU6u6y47VhEXkV6B9g1m3GmGesZW4D2oA/WvN2ADXGmH0iMgV4WkTGELhkyITY9yKcRVnU1NREfhBKKaW8xC1oGGPmhJovIpcB84HZVpETxpgWoMV6vUpE6oHhOHMWnkVY1cD2EPteAiwBqKurCxpclFJKhSdZrafmAjcDZxtjjnhMLxeRLOv1YJwV3huNMTuARhGZZrWauhR4JuHpTvQOlVIqxcQtp9GFXwB5wCtWPcEyq6XUScD3RaQNaAeuNsbst9a5Bvg9UICzDuRF340qpZSKr6QEDWPM0CDTnwCeCDJvJTA2nunqitaDK6W6u1RoPaWUUipNaNBQSillmwaNMOiT+5RS3Z0GDaWUUrZp0AiDVoQrpbo7DRpKKaVs06ChlFLKNg0aSimlbNOgoZRSyjYNGja4KsC1Ilwp1d1p0LBBY4VSSjlp0LBBH76klFJOGjRsEPf/GjyUUt2bBg0bXBkNE/xhgUop1S1o0LDBlcMwGjOUUt2cBg0beuRlJTsJSimVEpL15L608r9Xz+AfH+0iP0eDh1Kqe9OgYcPQvj0Z2rdnspOhlFJJp8VTSimlbNOgoZRSyjYNGkoppWzToKGUUso2DRpKKaVs06ChlFLKNg0aSimlbNOgoZRSyjYxGT6gkojsATZHuHofYG8Mk5MO9Ji7Bz3m7iGaYx5ojCn3nZjxQSMaIrLSGFOX7HQkkh5z96DH3D3E45i1eEoppZRtGjSUUkrZpkEjtCXJTkAS6DF3D3rM3UPMj1nrNJRSStmmOQ2llFK2adAIQETmisg6EdkgIrckOz2xIiIDROSfIvKRiKwVka9b03uJyCsist76v8xjncXW57BORM5IXuqjIyJZIvKuiDxnvc/oYxaRUhH5q4h8bH3f07vBMX/TOq/XiMhjIpKfaccsIr8Vkd0issZjWtjHKCJTROQDa959IiK2E2GM0T+PPyALqAcGA7nAamB0stMVo2OrACZbr4uAT4DRwD3ALdb0W4D/tl6Pto4/D6i1PpesZB9HhMf+H8CfgOes9xl9zMDDwFes17lAaSYfM1AFfAoUWO//AlyeaccMnARMBtZ4TAv7GIHlwHRAgBeBz9lNg+Y0/E0FNhhjNhpjWoHHgQVJTlNMGGN2GGPesV43Ah/h/LEtwHmRwfr/HOv1AuBxY0yLMeZTYAPOzyetiEg1MA/4tcfkjD1mESnGeXH5DYAxptUYc5AMPmZLNlAgItlAIbCdDDtmY8y/gP0+k8M6RhGpAIqNMUuNM4I84rFOlzRo+KsCtnq8b7CmZRQRGQRMAv4N9DPG7ABnYAH6WotlymfxP8BNQIfHtEw+5sHAHuB3VpHcr0WkBxl8zMaYbcC9wBZgB3DIGPMyGXzMHsI9xirrte90WzRo+AtUtpdRTcxEpCfwBPANY8zhUIsGmJZWn4WIzAd2G2NW2V0lwLS0Omacd9yTgQeMMZOAJpzFFsGk/TFb5fgLcBbDVAI9ROTiUKsEmJZWx2xDsGOM6tg1aPhrAAZ4vK/Gmc3NCCKSgzNg/NEY86Q1eZeVZcX6f7c1PRM+ixOAs0VkE86ixlNF5A9k9jE3AA3GmH9b7/+KM4hk8jHPAT41xuwxxhwDngRmkNnH7BLuMTZYr32n26JBw98KYJiI1IpILnAB8GyS0xQTVguJ3wAfGWN+4jHrWeAy6/VlwDMe0y8QkTwRqQWG4axASxvGmMXGmGpjzCCc3+VrxpiLyexj3glsFZER1qTZwIdk8DHjLJaaJiKF1nk+G2edXSYfs0tYx2gVYTWKyDTrs7rUY52uJbs1QCr+AWfibFlUD9yW7PTE8Lhm4syGvg+8Z/2dCfQG/gGst/7v5bHObdbnsI4wWlik4h9wMp2tpzL6mIGJwErru34aKOsGx/w94GNgDfAozlZDGXXMwGM462yO4cwxXBnJMQJ11udUD/wCq6O3nT/tEa6UUso2LZ5SSillmwYNpZRStmnQUEopZZsGDaWUUrZp0FBKKWWbBg2llFK2adBQSillmwYNpZRStv0/sZovtYo3M/UAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "environment = CliffWorld()\n",
    "agent = SARSALambdaAgent(alpha=0.1, epsilon=0.1, discount=0.99,\n",
    "                   get_legal_actions=environment.get_possible_actions, lambda_value = 0.5)\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(environment, agent))\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Zadanie 2 - Double Q-Learning\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenie jest zaimplementowanie algorytmu Double Q-Learning. Algorytm aktualizuje funkcję wartości stanu-akcji zgodnie ze wzorami:\n",
    "    \\begin{equation*}\n",
    "        Q_1(s_t, a_t) = Q_1(s_t, a_t) + \\alpha[r_{t+1} + \\gamma Q_2(s_{t + 1}, argmax_a(Q_1(s_{t + 1}, a))) - Q_1(s_t, a_t)]\n",
    "    \\end{equation*}\n",
    "    \\begin{equation*}\n",
    "        Q_2(s_t, a_t) = Q_2(s_t, a_t) + \\alpha[r_{t+1} + \\gamma Q_1(s_{t + 1}, argmax_a(Q_2(s_{t + 1}, a))) - Q_2(s_t, a_t)]\n",
    "    \\end{equation*}\n",
    "z prawdopodobieństwem wyboru każdej z opcji równym 50%.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class DQLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
    "        \"\"\"\n",
    "        Double Q-Learning Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvaluesA = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self._qvaluesB = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        \n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvaluesA[state][action] + self._qvaluesB[state][action] \n",
    "\n",
    "\n",
    "    #---------------------START OF YOUR CODE---------------------#\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        You should do your Q-Value update here\n",
    "        \"\"\"\n",
    "\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "        learning_rate = self.alpha\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to update value in the state for the action \n",
    "        #\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.\n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "\n",
    "        Note: To pick randomly from a list, use random.choice(list).\n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "\n",
    "        # Pick Action\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters:\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to get action in a given state (according to epsilon greedy algorithm)\n",
    "        #        \n",
    "\n",
    "        return chosen_action\n",
    "\n",
    "    def turn_off_learning(self):\n",
    "        self.epsilon = 0\n",
    "        self.alpha = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = dqnSimpleMDP()\n",
    "\n",
    "max_tests = 10000\n",
    "n_eps = 300\n",
    "eps = 0.1\n",
    "lr = 0.1\n",
    "\n",
    "\n",
    "left_count_q = np.zeros(n_eps) #count left choices in state A for algorithm Q-Learning\n",
    "left_count_dq = np.zeros(n_eps) #count left choices in state A for algorithm Double Q-Learning\n",
    "\n",
    "q_estimate = np.zeros(n_eps) #store value estimation for choosing action left in left choices in state A for algorithm Q-Learning\n",
    "dq_estimate = np.zeros(n_eps) #store value estimation for choosing action left in left choices in state A for algorithm Double Q-Learning\n",
    "\n",
    "t = 0\n",
    "while t < max_tests:\n",
    "\n",
    "    agent = QLearningAgent(alpha=0.1, epsilon=0.1, discount=1,\n",
    "                           get_legal_actions=env.get_possible_actions)\n",
    "    for ep in range(n_eps):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            # Select eps-greedy action\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # Count left actions from A\n",
    "            if state == 'A' and action == 1:\n",
    "                left_count_q[ep] += 1\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            agent.update(state, action, reward, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                q_estimate[ep] += agent.get_qvalue('A', 1)\n",
    "\n",
    "                break\n",
    "    t += 1\n",
    "\n",
    "q_estimate /= max_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t = 0\n",
    "while t < max_tests:\n",
    "\n",
    "    agent = DQLearningAgent(alpha=0.1, epsilon=0.1, discount=1,\n",
    "                            get_legal_actions=env.get_possible_actions)\n",
    "    for ep in range(n_eps):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            # Select eps-greedy action\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # Count left actions from A\n",
    "            if state == 'A' and action == 1:\n",
    "                left_count_dq[ep] += 1\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            agent.update(state, action, reward, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                dq_estimate[ep] += agent.get_qvalue('A', 1) / 2\n",
    "\n",
    "                break\n",
    "    t += 1\n",
    "\n",
    "dq_estimate /= max_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(left_count_q/max_tests*100,\n",
    "         label='Q-Learning')\n",
    "plt.plot(left_count_dq/max_tests*100,\n",
    "         label='Double Q-Learning')\n",
    "plt.ylabel('Percentage of Left Actions')\n",
    "plt.xlabel('Episodes')\n",
    "plt.title(r'Q-Learning Action Selection ($\\epsilon=0.1$)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(q_estimate, label='Q-Learning')\n",
    "plt.plot(dq_estimate, label='Double Q-Learning')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Estimated Value')\n",
    "plt.title('Estimated Value of Choosing Left from State A')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Zadanie 3 \n",
    "\n",
    "Przetestuj działanie wszystkich zaimplementowanych algorytmów w środowisku dqnSimpleMDP, który algorytm działa najlepiej?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
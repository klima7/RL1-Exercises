{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Laboratorium 3 (4 pkt.)\n",
    "\n",
    "Celem trzeciego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmów uczenia aktywnego. Zaimplementowane algorytmy będą testowane z wykorzystaniem wcześniej przygotowanych środowisk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dołączenie standardowych bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dołączenie bibliotek ze środowiskami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from env.dqnSimpleMDP import dqnSimpleMDP\n",
    "from env.CliffWorldMDP import CliffWorld"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie 1 - SARSA($\\lambda$) (1 pkt.)\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenie jest zaimplementowanie algorytmu SARSA($\\lambda$). Algorytm aktualizuje funkcję wartości stanu-akcji dla każdej odwiedzonej pary stan-akcja zgodnie ze wzorem:\n",
    "\\begin{equation}\n",
    "    Q_{t + 1}(s, a) = Q_t(s, a) + \\alpha \\delta_t E_t(s, a)\n",
    "\\end{equation}\n",
    "gdzie:\n",
    "    \n",
    "- $\\delta_t = r_{t + 1} \\gamma Q_t(s_{t + 1}, a_{t + 1}) - Q_t(s_{t}, a_{t})$,\n",
    "    \n",
    "- $E_t(s, a)$ - ślad dla pary stan - akcja w chwili czasowej $t$, zwiększany o $1$ w chwili odwiedzenia danego stanu.\n",
    "\n",
    "</p>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class SARSALambdaAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions, lambda_value):\n",
    "        \"\"\"\n",
    "        SARSA Lambda Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "\n",
    "        Functions you should use\n",
    "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
    "            which returns legal actions for a state\n",
    "          - self.get_qvalue(state,action)\n",
    "            which returns Q(state,action)\n",
    "          - self.set_qvalue(state,action,value)\n",
    "            which sets Q(state,action) := value\n",
    "        !!!Important!!!\n",
    "        Note: please avoid using self._qValues directly.\n",
    "            There's a special self.get_qvalue/set_qvalue for that.\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self._evalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self._visited_state_actions = []\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.lambda_value = lambda_value\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
    "        self._qvalues[state][action] = value\n",
    "\n",
    "    def reset(self):\n",
    "        self._evalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self._visited_state_actions = []\n",
    "\n",
    "    # ---------------------START OF YOUR CODE---------------------#\n",
    "    def get_evalue(self, state, action):\n",
    "        \"\"\" Returns E(state,action) \"\"\"\n",
    "        return self._evalues[state][action]\n",
    "\n",
    "    def set_evalue(self, state, action, value):\n",
    "        \"\"\" Sets the Evalue for [state,action] to the given value \"\"\"\n",
    "        self._evalues[state][action] = value\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        You should do your SARSA-Lambda update here:\n",
    "        \"\"\"\n",
    "\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to update value in the state for the action \n",
    "        #\n",
    "        next_action = self.get_action(next_state)\n",
    "\n",
    "        delta = reward + gamma * self.get_qvalue(next_state, next_action) - self.get_qvalue(state, action)\n",
    "        self._update_current_state_action_trace(state, action)\n",
    "        self._update_past_state_actions(delta)\n",
    "\n",
    "        return next_action\n",
    "\n",
    "    def _update_current_state_action_trace(self, state, action):\n",
    "        current_value = self.get_evalue(state, action)\n",
    "        new_value = current_value + 1\n",
    "        self.set_evalue(state, action, new_value)\n",
    "        self._visited_state_actions.append((state, action))\n",
    "\n",
    "    def _update_past_state_actions(self, delta):\n",
    "        for state, action in self._visited_state_actions:\n",
    "            e_value = self.get_evalue(state, action)\n",
    "            q_value = self.get_qvalue(state, action)\n",
    "            new_qvalue = q_value + self.alpha * delta * e_value\n",
    "            new_evalue = self.discount * self.alpha * e_value\n",
    "\n",
    "            self.set_qvalue(state, action, new_qvalue)\n",
    "            self.set_evalue(state, action, new_evalue)\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the best action to take in a state (using current q-values).\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to get best action for a given state\n",
    "        #\n",
    "        #\n",
    "        qvalues = [self.get_qvalue(state, action) for action in possible_actions]\n",
    "        best_qvalue = max(qvalues)\n",
    "        best_actions = [action for action, qvalue in zip(possible_actions, qvalues) if qvalue == best_qvalue]\n",
    "        best_action = random.choice(best_actions)\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.\n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "\n",
    "        Note: To pick randomly from a list, use random.choice(list).\n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "\n",
    "        # Pick Action\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters:\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to get action in a given state (according to epsilon greedy algorithm)\n",
    "        #\n",
    "        should_random = random.random() < epsilon\n",
    "\n",
    "        if should_random:\n",
    "            chosen_action = random.choice(possible_actions)\n",
    "        else:\n",
    "            return self.get_best_action(state)\n",
    "\n",
    "        return chosen_action\n",
    "\n",
    "    def turn_off_learning(self):\n",
    "        self.epsilon = 0\n",
    "        self.alpha = 0\n",
    "\n",
    "    def display_qvalues(self):\n",
    "        for s in self._qvalues:\n",
    "            print(\"State: \" + str(s) + \" \" + str(self._qvalues[s]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Czas nauczyć agenta poruszania się po dowolnym środowisku:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def play_and_train(env, agent):\n",
    "    \"\"\"\n",
    "    This function should\n",
    "    - run a full game, actions given by agent's e-greedy policy\n",
    "    - train agent using agent.update(...) whenever it is possible\n",
    "    - return total reward\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    state = env.reset()\n",
    "\n",
    "    agent.reset()\n",
    "\n",
    "    done = False\n",
    "    action = agent.get_action(state)\n",
    "\n",
    "    while not done:\n",
    "        # get agent to pick action given state state.\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # train (update) agent for state\n",
    "        action = agent.update(state, action, reward, next_state)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "environment = CliffWorld()\n",
    "agent = SARSALambdaAgent(alpha=0.1, epsilon=0.1, discount=0.99,\n",
    "                   get_legal_actions=environment.get_possible_actions, lambda_value = 0.5)\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(environment, agent))\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie 2 - Double Q-Learning\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenie jest zaimplementowanie algorytmu Double Q-Learning. Algorytm aktualizuje funkcję wartości stanu-akcji zgodnie ze wzorami:\n",
    "    \\begin{equation*}\n",
    "        Q_1(s_t, a_t) = Q_1(s_t, a_t) + \\alpha[r_{t+1} + \\gamma Q_2(s_{t + 1}, argmax_a(Q_1(s_{t + 1}, a))) - Q_1(s_t, a_t)]\n",
    "    \\end{equation*}\n",
    "    \\begin{equation*}\n",
    "        Q_2(s_t, a_t) = Q_2(s_t, a_t) + \\alpha[r_{t+1} + \\gamma Q_1(s_{t + 1}, argmax_a(Q_2(s_{t + 1}, a))) - Q_2(s_t, a_t)]\n",
    "    \\end{equation*}\n",
    "z prawdopodobieństwem wyboru każdej z opcji równym 50%.\n",
    "</p>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class DQLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
    "        \"\"\"\n",
    "        Double Q-Learning Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvaluesA = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self._qvaluesB = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        \n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvaluesA[state][action] + self._qvaluesB[state][action] \n",
    "\n",
    "\n",
    "    #---------------------START OF YOUR CODE---------------------#\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        You should do your Q-Value update here\n",
    "        \"\"\"\n",
    "        #\n",
    "        # INSERT CODE HERE to update value in the state for the action \n",
    "        #\n",
    "        gamma = self.discount\n",
    "        learning_rate = self.alpha\n",
    "\n",
    "        q1, q2 = random.choice([\n",
    "            [self._qvaluesA, self._qvaluesB],\n",
    "            [self._qvaluesB, self._qvaluesA]\n",
    "        ])\n",
    "\n",
    "        q1[state][action] = (1-learning_rate) * q1[state][action] + learning_rate * (reward + gamma * self.get_value(q1, q2, next_state))\n",
    "\n",
    "    def get_value(self, q1, q2, state):\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        actions_values = [q1[state][action] for action in possible_actions]\n",
    "        max_action_value = max(actions_values)\n",
    "        best_actions = [action for action, value in zip(possible_actions, actions_values) if value == max_action_value]\n",
    "        best_action = random.choice(best_actions)\n",
    "\n",
    "        return q2[state][best_action]\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to get best possible action in a given state (remember to break ties randomly)\n",
    "        #\n",
    "        qvalues = [self.get_qvalue(state, action) for action in possible_actions]\n",
    "        best_qvalue = max(qvalues)\n",
    "        best_actions = [action for action, qvalue in zip(possible_actions, qvalues) if qvalue == best_qvalue]\n",
    "        best_action = random.choice(best_actions)\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.\n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "\n",
    "        Note: To pick randomly from a list, use random.choice(list).\n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "\n",
    "        # Pick Action\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters:\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to get action in a given state (according to epsilon greedy algorithm)\n",
    "        #        \n",
    "        should_random = random.random() < epsilon\n",
    "\n",
    "        if should_random:\n",
    "            chosen_action = random.choice(possible_actions)\n",
    "        else:\n",
    "            return self.get_best_action(state)\n",
    "\n",
    "        return chosen_action\n",
    "\n",
    "    def turn_off_learning(self):\n",
    "        self.epsilon = 0\n",
    "        self.alpha = 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "env = dqnSimpleMDP()\n",
    "\n",
    "max_tests = 10000\n",
    "n_eps = 300\n",
    "eps = 0.1\n",
    "lr = 0.1\n",
    "\n",
    "\n",
    "left_count_q = np.zeros(n_eps) #count left choices in state A for algorithm Q-Learning\n",
    "left_count_dq = np.zeros(n_eps) #count left choices in state A for algorithm Double Q-Learning\n",
    "\n",
    "q_estimate = np.zeros(n_eps) #store value estimation for choosing action left in left choices in state A for algorithm Q-Learning\n",
    "dq_estimate = np.zeros(n_eps) #store value estimation for choosing action left in left choices in state A for algorithm Double Q-Learning\n",
    "\n",
    "t = 0\n",
    "while t < max_tests:\n",
    "\n",
    "    agent = QLearningAgent(alpha=0.1, epsilon=0.1, discount=1,\n",
    "                           get_legal_actions=env.get_possible_actions)\n",
    "    for ep in range(n_eps):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            # Select eps-greedy action\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # Count left actions from A\n",
    "            if state == 'A' and action == 1:\n",
    "                left_count_q[ep] += 1\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            agent.update(state, action, reward, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                q_estimate[ep] += agent.get_qvalue('A', 1)\n",
    "\n",
    "                break\n",
    "    t += 1\n",
    "\n",
    "q_estimate /= max_tests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "t = 0\n",
    "while t < max_tests:\n",
    "\n",
    "    agent = DQLearningAgent(alpha=0.1, epsilon=0.1, discount=1,\n",
    "                            get_legal_actions=env.get_possible_actions)\n",
    "    for ep in range(n_eps):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            # Select eps-greedy action\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # Count left actions from A\n",
    "            if state == 'A' and action == 1:\n",
    "                left_count_dq[ep] += 1\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            agent.update(state, action, reward, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                dq_estimate[ep] += agent.get_qvalue('A', 1) / 2\n",
    "\n",
    "                break\n",
    "    t += 1\n",
    "\n",
    "dq_estimate /= max_tests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(left_count_q/max_tests*100,\n",
    "         label='Q-Learning')\n",
    "plt.plot(left_count_dq/max_tests*100,\n",
    "         label='Double Q-Learning')\n",
    "plt.ylabel('Percentage of Left Actions')\n",
    "plt.xlabel('Episodes')\n",
    "plt.title(r'Q-Learning Action Selection ($\\epsilon=0.1$)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(q_estimate, label='Q-Learning')\n",
    "plt.plot(dq_estimate, label='Double Q-Learning')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Estimated Value')\n",
    "plt.title('Estimated Value of Choosing Left from State A')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
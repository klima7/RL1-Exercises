{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 3 (4 pkt.)\n",
    "\n",
    "Celem trzeciego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmów uczenia aktywnego. Zaimplementowane algorytmy będą testowane z wykorzystaniem wcześniej przygotowanych środowisk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dołączenie standardowych bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dołączenie bibliotek ze środowiskami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.dqnSimpleMDP import dqnSimpleMDP\n",
    "from env.CliffWorldMDP import CliffWorld\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1 - SARSA($\\lambda$) (1 pkt.)\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenie jest zaimplementowanie algorytmu SARSA($\\lambda$). Algorytm aktualizuje funkcję wartości stanu-akcji dla każdej odwiedzonej pary stan-akcja zgodnie ze wzorem:\n",
    "\\begin{equation}\n",
    "    Q_{t + 1}(s, a) = Q_t(s, a) + \\alpha \\delta_t E_t(s, a)\n",
    "\\end{equation}\n",
    "gdzie:\n",
    "    \n",
    "- $\\delta_t = r_{t + 1} \\gamma Q_t(s_{t + 1}, a_{t + 1}) - Q_t(s_{t}, a_{t})$,\n",
    "    \n",
    "- $E_t(s, a)$ - ślad dla pary stan - akcja w chwili czasowej $t$, zwiększany o $1$ w chwili odwiedzenia danego stanu.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class SARSALambdaAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions, lambda_value):\n",
    "        \"\"\"\n",
    "        SARSA Lambda Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "\n",
    "        Functions you should use\n",
    "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
    "            which returns legal actions for a state\n",
    "          - self.get_qvalue(state,action)\n",
    "            which returns Q(state,action)\n",
    "          - self.set_qvalue(state,action,value)\n",
    "            which sets Q(state,action) := value\n",
    "        !!!Important!!!\n",
    "        Note: please avoid using self._qValues directly.\n",
    "            There's a special self.get_qvalue/set_qvalue for that.\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self._evalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.lambda_value = lambda_value\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
    "        self._qvalues[state][action] = value\n",
    "\n",
    "    def reset(self):\n",
    "        self._evalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    # ---------------------START OF YOUR CODE---------------------#\n",
    "\n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "        Compute your agent's estimate of V(s) using current q-values\n",
    "        V(s) = max_over_action Q(state,action) over possible actions.\n",
    "        Note: please take into account that q-values can be negative.\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to get maximum possible value for a given state\n",
    "        #\n",
    "\n",
    "\n",
    "        return max(values)\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        You should do your SARSA-Lambda update here:\n",
    "        \"\"\"\n",
    "\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "        learning_rate = self.alpha\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to update value in the state for the action \n",
    "        #\n",
    "\n",
    "        return next_action\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the best action to take in a state (using current q-values).\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to get best action for a given state\n",
    "        #\n",
    "        #\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.\n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "\n",
    "        Note: To pick randomly from a list, use random.choice(list).\n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "\n",
    "        # Pick Action\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters:\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to get action in a given state (according to epsilon greedy algorithm)\n",
    "        #        \n",
    "\n",
    "        return chosen_action\n",
    "\n",
    "    def turn_off_learning(self):\n",
    "        self.epsilon = 0\n",
    "        self.alpha = 0\n",
    "\n",
    "    def display_qvalues(self):\n",
    "        for s in self._qvalues:\n",
    "            print(\"State: \" + str(s) + \" \" + str(self._qvalues[s]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Czas nauczyć agenta poruszania się po dowolnym środowisku:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_train(env, agent):\n",
    "    \"\"\"\n",
    "    This function should\n",
    "    - run a full game, actions given by agent's e-greedy policy\n",
    "    - train agent using agent.update(...) whenever it is possible\n",
    "    - return total reward\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "\n",
    "    agent.reset()\n",
    "    action = agent.get_action(state)\n",
    "    \n",
    "    while not done:\n",
    "        # get agent to pick action given state state.\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # train (update) agent for state\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = CliffWorld()\n",
    "agent = SARSALambdaAgent(alpha=0.1, epsilon=0.1, discount=0.99,\n",
    "                   get_legal_actions=environment.get_possible_actions, lambda_value = 0.5)\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(environment, agent))\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2 - Double Q-Learning\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Celem ćwiczenie jest zaimplementowanie algorytmu Double Q-Learning. Algorytm aktualizuje funkcję wartości stanu-akcji zgodnie ze wzorami:\n",
    "    \\begin{equation*}\n",
    "        Q_1(s_t, a_t) = Q_1(s_t, a_t) + \\alpha[r_{t+1} + \\gamma Q_2(s_{t + 1}, argmax_a(Q_1(s_{t + 1}, a))) - Q_1(s_t, a_t)]\n",
    "    \\end{equation*}\n",
    "    \\begin{equation*}\n",
    "        Q_2(s_t, a_t) = Q_2(s_t, a_t) + \\alpha[r_{t+1} + \\gamma Q_1(s_{t + 1}, argmax_a(Q_2(s_{t + 1}, a))) - Q_2(s_t, a_t)]\n",
    "    \\end{equation*}\n",
    "z prawdopodobieństwem wyboru każdej z opcji równym 50%.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class DQLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
    "        \"\"\"\n",
    "        Double Q-Learning Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvaluesA = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self._qvaluesB = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        \n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvaluesA[state][action] + self._qvaluesB[state][action] \n",
    "\n",
    "\n",
    "    #---------------------START OF YOUR CODE---------------------#\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        You should do your Q-Value update here\n",
    "        \"\"\"\n",
    "\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "        learning_rate = self.alpha\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to update value in the state for the action \n",
    "        #\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.\n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "\n",
    "        Note: To pick randomly from a list, use random.choice(list).\n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "\n",
    "        # Pick Action\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters:\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        #\n",
    "        # INSERT CODE HERE to get action in a given state (according to epsilon greedy algorithm)\n",
    "        #        \n",
    "\n",
    "        return chosen_action\n",
    "\n",
    "    def turn_off_learning(self):\n",
    "        self.epsilon = 0\n",
    "        self.alpha = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = dqnSimpleMDP()\n",
    "\n",
    "max_tests = 10000\n",
    "n_eps = 300\n",
    "eps = 0.1\n",
    "lr = 0.1\n",
    "\n",
    "\n",
    "left_count_q = np.zeros(n_eps) #count left choices in state A for algorithm Q-Learning\n",
    "left_count_dq = np.zeros(n_eps) #count left choices in state A for algorithm Double Q-Learning\n",
    "\n",
    "q_estimate = np.zeros(n_eps) #store value estimation for choosing action left in left choices in state A for algorithm Q-Learning\n",
    "dq_estimate = np.zeros(n_eps) #store value estimation for choosing action left in left choices in state A for algorithm Double Q-Learning\n",
    "\n",
    "t = 0\n",
    "while t < max_tests:\n",
    "\n",
    "    agent = QLearningAgent(alpha=0.1, epsilon=0.1, discount=1,\n",
    "                           get_legal_actions=env.get_possible_actions)\n",
    "    for ep in range(n_eps):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            # Select eps-greedy action\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # Count left actions from A\n",
    "            if state == 'A' and action == 1:\n",
    "                left_count_q[ep] += 1\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            agent.update(state, action, reward, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                q_estimate[ep] += agent.get_qvalue('A', 1)\n",
    "\n",
    "                break\n",
    "    t += 1\n",
    "\n",
    "q_estimate /= max_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "while t < max_tests:\n",
    "\n",
    "    agent = DQLearningAgent(alpha=0.1, epsilon=0.1, discount=1,\n",
    "                            get_legal_actions=env.get_possible_actions)\n",
    "    for ep in range(n_eps):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            # Select eps-greedy action\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # Count left actions from A\n",
    "            if state == 'A' and action == 1:\n",
    "                left_count_dq[ep] += 1\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            agent.update(state, action, reward, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                dq_estimate[ep] += agent.get_qvalue('A', 1) / 2\n",
    "\n",
    "                break\n",
    "    t += 1\n",
    "\n",
    "dq_estimate /= max_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(left_count_q/max_tests*100,\n",
    "         label='Q-Learning')\n",
    "plt.plot(left_count_dq/max_tests*100,\n",
    "         label='Double Q-Learning')\n",
    "plt.ylabel('Percentage of Left Actions')\n",
    "plt.xlabel('Episodes')\n",
    "plt.title(r'Q-Learning Action Selection ($\\epsilon=0.1$)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(q_estimate, label='Q-Learning')\n",
    "plt.plot(dq_estimate, label='Double Q-Learning')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Estimated Value')\n",
    "plt.title('Estimated Value of Choosing Left from State A')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3 \n",
    "\n",
    "Przetestuj działanie wszystkich zaimplementowanych algorytmów w środowisku dqnSimpleMDP, który algorytm działa najlepiej?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
